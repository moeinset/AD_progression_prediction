{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeinset/AD_progression_prediction/blob/main/alzheimer_progression_by_neuroimaging_and.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:28.633613Z",
          "iopub.execute_input": "2022-09-30T14:37:28.63401Z",
          "iopub.status.idle": "2022-09-30T14:37:28.642594Z",
          "shell.execute_reply.started": "2022-09-30T14:37:28.633979Z",
          "shell.execute_reply": "2022-09-30T14:37:28.641275Z"
        },
        "trusted": true,
        "id": "5C12gvN0FCcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m87OcQMWFCcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALZHEIMER PROGRESSION PREDICTION WITH NEUROIMAGING AND COGNITIVE TESTS;A MACHINE LEARNING MODELS STUDY**\n",
        "Alzheimer’s disease (AD)is the driving cause of dementia in older adults. Their frequency rates are expanding at an disturbing rate each year. A treatment given at an early stage of AD is more effective, and it causes less minor harm than a treatment done at a later stage.so in this study we try machine leanring method to train a model for predict the dementia progression with just  use neuroimaging  and cognitive tests at baseline."
      ],
      "metadata": {
        "id": "aqBaalXaFCc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this training We use AD datasets collected from the Alzheimer’s Disease Neuroimaging Initiative (ADNI).the raw dataset(Merg,all collected data) were preprocessing used python and libraries like pandas in my notebook! I removed non imaging features like bood biomarkers and .. . the first cleaned data imported and convert all columns heading to lower case for better manipulating. let take a brief look use describe() method!  "
      ],
      "metadata": {
        "id": "DdDIGYQSFCc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas for datacleaning\n",
        "import pandas as pd\n",
        "\n",
        "#import dataset\n",
        "#this dataset is previously cleaned, so it include only neuroimaging,cognitive scores and demographics features\n",
        "#also missing values in DX feature were droped by pandas.dropna() command becaus this will be our Target column.\n",
        "df = pd.read_csv('../input/alzheimer-just-neuroimaging-and-cogtest-baseline/first_cleaned_data.csv')\n",
        "df = pd.DataFrame(df) #convert data to dataframe\n",
        "df.columns = df.columns.str.lower()#convert all columns heading to lower case for better manipulating\n",
        "df.describe() #firs look!"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:28.652057Z",
          "iopub.execute_input": "2022-09-30T14:37:28.652499Z",
          "iopub.status.idle": "2022-09-30T14:37:29.109612Z",
          "shell.execute_reply.started": "2022-09-30T14:37:28.652462Z",
          "shell.execute_reply": "2022-09-30T14:37:29.107843Z"
        },
        "trusted": true,
        "id": "fTlcczPpFCc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**summary data description**\n",
        "as you can see we have 11195 samples that include different participants(rid), with different visit session(visitcode). features data are demographics, neuroimaging and cognitive tests. descriptive statistics are shown by describe() method up there."
      ],
      "metadata": {
        "id": "vSR5yPQ2FCc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STUDY DESCRIPTION**\n",
        "We try to train a model(s) that can predict the progression of participants cognitive status using very non-invasive procedures(even blood biomarkers were exclude!).\n",
        "\n",
        "**TARGET**\n",
        "to aim this we should first set the target. Our target for this study is the progression of participants dementia status from baseline. We just have diagnostic status for each visit session.CN for cognitive normal, MCI for mild cognitive impairment and Dementia for Alzheimer. We dont have target directly in data, so we need to extract this from data by compare the baseline diagnosis and last session diagnosis(at least 6 month later).we have different visit time session for participants and we consider the last one(from 6 to 186 month).\n",
        "\n",
        "**TARGET EXTRACTION ALGORITHM**\n",
        "1-assign a numeric value to visit session(visitcode)(e.g. baseline = 1, month 6 = 2 and ...).\n",
        "2-filter and split dataset by first visit(minimum value) and last visit(maximum value)\n",
        "3-campare the diagnosis status(dx columne):\n",
        "1. if baseline is normal and still stay normal = low risk dementia progression(target = HCL)\n",
        "1. if baseline is normal and progress to MCI  = risk of MCI(target = mMCI)\n",
        "1. if baseline is normal and progress to DEMENTIA  = risk of AD(target = AD)\n",
        "1. if baseline is MCI and still stay MCI  = stable MCI(target = sMCI)\n",
        "1. if baseline is MCI and progress to DEMENTIA  = progressive MCI(target = pMCI)\n",
        "1. if baseline is MCI and last vist is normal!  = data collection bias(remove data)"
      ],
      "metadata": {
        "id": "A43Yd3hJFCc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['viscode'].unique() \n",
        "df[\"viscode\"].replace({\"bl\": \"1\", \"m06\": \"2\",'m12': \"3\", 'm24': \"4\", 'm18': \"5\", 'm36': \"6\", 'm48': \"7\", 'm60': \"8\", 'm72': \"9\",\n",
        "       'm108': \"10\", 'm96': \"11\", 'm84': \"12\", 'm120': \"13\", 'm132': \"14\", 'm144': \"15\", 'm156': \"16\", 'm168': \"17\",\n",
        "       'm180': \"18\", 'm42': \"19\", 'm174': \"20\", 'm162': \"21\", 'm138': \"22\", 'm150' : \"23\", 'm102': \"24\", 'm90': \"25\",\n",
        "       'm114': \"26\", 'm78': \"27\", 'm126': \"28\", 'm66': \"29\", 'm54': \"30\", 'm30': \"31\", 'm186': \"32\"}, inplace=True)\n",
        "df[\"rid\"] = pd.to_numeric(df[\"rid\"])\n",
        "df[\"viscode\"] = pd.to_numeric(df[\"viscode\"])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.112489Z",
          "iopub.execute_input": "2022-09-30T14:37:29.113025Z",
          "iopub.status.idle": "2022-09-30T14:37:29.146532Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.112977Z",
          "shell.execute_reply": "2022-09-30T14:37:29.145383Z"
        },
        "trusted": true,
        "id": "psR7V99pFCdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfmin = df.loc[df.groupby('rid')['viscode'].idxmin()]\n",
        "dfmin.rename(columns={'dx' : 'dx_min'}, inplace=True)\n",
        "dfmin = dfmin[dfmin['dx_min'] != 'Dementia']\n",
        "dfmax = df.loc[df.groupby('rid')['viscode'].idxmax()]\n",
        "dfmax = dfmax[dfmax['viscode'] >= 2]\n",
        "dfmax.rename(columns={'dx' : 'dx_max'}, inplace=True)\n",
        "df2 = pd.merge(dfmin,dfmax[['rid','dx_max']],on='rid', how='left')\n",
        "\n",
        "conditions = [\n",
        "    (df2['dx_min'] == 'CN') & (df2['dx_max'] == 'CN'),\n",
        "    (df2['dx_min'] == 'CN') & (df2['dx_max'] == 'MCI'),\n",
        "    (df2['dx_min'] == 'CN') & (df2['dx_max'] == 'Dementia'),\n",
        "    (df2['dx_min'] == 'MCI') & (df2['dx_max'] == 'CN'),\n",
        "    (df2['dx_min'] == 'MCI') & (df2['dx_max'] == 'MCI'),\n",
        "    (df2['dx_min'] == 'MCI') & (df2['dx_max'] == 'Dementia')\n",
        "    ]\n",
        "\n",
        "# create a list of the values we want to assign for each condition\n",
        "values = ['HCL', 'mMCI', 'AD', '','sMCI','pMCI']\n",
        "\n",
        "# create a new column and use np.select to assign values to it using our lists as arguments\n",
        "df2['target'] = np.select(conditions, values)\n",
        "df2.drop(['dx_min','dx_max'],axis = 1, inplace = True)\n",
        "df2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.147907Z",
          "iopub.execute_input": "2022-09-30T14:37:29.148238Z",
          "iopub.status.idle": "2022-09-30T14:37:29.462952Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.148209Z",
          "shell.execute_reply": "2022-09-30T14:37:29.461812Z"
        },
        "trusted": true,
        "id": "Qtr-245NFCdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "at last we have 1969 samples. but be careful! this participants survey in different visit time. you can see them in visitcode columne of primary data frame(bl=baseline, m06=month 6 and ...). but we just need baseline datas. so we extracted them!"
      ],
      "metadata": {
        "id": "UwAnOWZBFCdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #because we want to use just baseline features we should filter non-baseline data\n",
        "df3 = df2[df2['viscode']== 1] #extract basle line by visitcod(from bl(baseline),m06(month 6) , ...)\n",
        "df4 = df3.drop(['viscode','ptid','rid','fsversion_bl','fsversion','examdate_bl'], axis = 1) #drop some non usfull! features\n",
        "bl_list = df4.filter(regex='bl').columns #make a list in columns that have bl key(baseline)\n",
        "non_bl_list = [s.replace(\"_bl\", \"\") for s in bl_list]#make a list of columns name that used to have bl in them(basle line)\n",
        "df5 = df4.drop(columns=non_bl_list) #remove the columns those arent baseline\n",
        "df5 = df5[df5['target'] != '0']\n",
        "df5 = df5[df5['target'] != '']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.464423Z",
          "iopub.execute_input": "2022-09-30T14:37:29.464792Z",
          "iopub.status.idle": "2022-09-30T14:37:29.48146Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.464759Z",
          "shell.execute_reply": "2022-09-30T14:37:29.480396Z"
        },
        "trusted": true,
        "id": "neNGEJV6FCdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "after baseline features extraction let take a look at missing values. in some data it would be better to first analysis noise and then missing values. but in this case seems noise features are not the issue."
      ],
      "metadata": {
        "id": "hjqCbJaRFCdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 100) #set display option maximum to 100\n",
        "miss = df5.isnull().sum().sort_values(ascending = False) #make a list of summarised sorted missing val\n",
        "miss = pd.DataFrame(miss) #convert them to dataframe\n",
        "miss.columns = ['MissValues'] #change columne name\n",
        "miss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.486747Z",
          "iopub.execute_input": "2022-09-30T14:37:29.487133Z",
          "iopub.status.idle": "2022-09-30T14:37:29.504127Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.487102Z",
          "shell.execute_reply": "2022-09-30T14:37:29.503053Z"
        },
        "trusted": true,
        "id": "9X2_PZ10FCdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you see we have a huge number of missing values. in usual case we should contact with data set owner for better deciding. but in this case we chose to drop feauturse with the critical missing values. in orther we get help from article review to NOT remove important and effective features. so we chose a cut point higher than important/effective features."
      ],
      "metadata": {
        "id": "wfo7uA2kFCdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### as you see we have a huge number of missing values. in usual case we should contact with \n",
        "#data set owner for better deciding. but in thid case we chose to drop feauturse with the \n",
        "#critical missing values. in orther we get help from article review to\n",
        "#NOT remove important and effective features\n",
        "miss = miss[miss['MissValues'] < 211]#extract features with less than 384 missing values\n",
        "#Considering no damage to important features\n",
        "final_featurs = miss.T.columns #convert the data frame rows to columns and make features list\n",
        "df6 = df5[final_featurs]#extract chosen features\n",
        "df6 #secound look!"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.5056Z",
          "iopub.execute_input": "2022-09-30T14:37:29.505939Z",
          "iopub.status.idle": "2022-09-30T14:37:29.552168Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.505908Z",
          "shell.execute_reply": "2022-09-30T14:37:29.551059Z"
        },
        "trusted": true,
        "id": "Pjy_qBwPFCdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 22 featurs! this amount of features is a double-edged sword! it can improve the accuracy but also increase the risk of overfitting. so we should extract the important features and reduce them by some method to minimize overfitting risk."
      ],
      "metadata": {
        "id": "uxCZyIOgFCdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURES SELECTION AND REDUCTION**\n",
        "In this level we should select features. first we used visualization and plots to select more **correlated features**. then used a machine learning algorithm called **Principal component analysis (PCA)** to reduced features. The existence of many features and little data has the risk of overfitting."
      ],
      "metadata": {
        "id": "MzgJYW_IFCdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets look closer at features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.553593Z",
          "iopub.execute_input": "2022-09-30T14:37:29.553931Z",
          "iopub.status.idle": "2022-09-30T14:37:29.55859Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.553901Z",
          "shell.execute_reply": "2022-09-30T14:37:29.557036Z"
        },
        "trusted": true,
        "id": "qg_sO4oKFCdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examining the Relationship between Features and Target**\n",
        "\n",
        "it is useful to be able to visualize how each feature affects the diagnosis. So let’s plot a histogram for each feature and then differentiate the cognitive progression using color:"
      ],
      "metadata": {
        "id": "KZAOYRDLFCdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a list of features\n",
        "df_fn = df6.drop(['target'],axis = 1)\n",
        "feature_names = df_fn.columns.T\n",
        "feature_names"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.560815Z",
          "iopub.execute_input": "2022-09-30T14:37:29.561353Z",
          "iopub.status.idle": "2022-09-30T14:37:29.57629Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.561284Z",
          "shell.execute_reply": "2022-09-30T14:37:29.574844Z"
        },
        "trusted": true,
        "id": "4nKMacsYFCdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analysis the target feature \n",
        "print(df6.target) \n",
        "print(feature_names) \n",
        "print(np.array(np.unique(df6.target, return_counts=True)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.578451Z",
          "iopub.execute_input": "2022-09-30T14:37:29.578997Z",
          "iopub.status.idle": "2022-09-30T14:37:29.590092Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.578959Z",
          "shell.execute_reply": "2022-09-30T14:37:29.588932Z"
        },
        "trusted": true,
        "id": "FmOm-l4FFCdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "_,axes = plt.subplots(5,5, figsize=(15, 15))\n",
        "HCL = df6[df6.target=='HCL'].to_numpy()\n",
        "mMCI = df6[df6.target=='mMCI'].to_numpy()\n",
        "sMCI = df6[df6.target=='sMCI'].to_numpy()\n",
        "pMCI = df6[df6.target=='pMCI'].to_numpy()\n",
        "AD = df6[df6.target=='AD'].to_numpy()\n",
        "\n",
        "\n",
        "ax = axes.ravel()                     # flatten the 2D arrayfor i in range(22):          \n",
        "for i in range(24):   # for each of the 30 features\n",
        "    bins = 40    #---plot histogram for each feature---\n",
        "    ax[i].hist(HCL[:,i], bins=bins, color='g', alpha=.5)\n",
        "    ax[i].hist(mMCI[:,i], bins=bins, color='b', alpha=0.3) #---set the title---\n",
        "    ax[i].hist(sMCI[:,i], bins=bins, color='y', alpha=.5)\n",
        "    ax[i].hist(pMCI[:,i], bins=bins, color='m', alpha=0.3)\n",
        "    ax[i].hist(AD[:,i], bins=bins, color='r', alpha=0.3)\n",
        "    ax[i].set_title(feature_names[i], fontsize=12)        #---display the legend---\n",
        "    ax[i].legend(['HCL','mMCI','sMCI','pMCI','AD'], loc='best', fontsize=8)\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:29.591508Z",
          "iopub.execute_input": "2022-09-30T14:37:29.591843Z",
          "iopub.status.idle": "2022-09-30T14:37:45.244637Z",
          "shell.execute_reply.started": "2022-09-30T14:37:29.591813Z",
          "shell.execute_reply": "2022-09-30T14:37:45.243775Z"
        },
        "trusted": true,
        "id": "UyXOFgB9FCdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this plots:\n",
        "\n",
        "1-we have imbalance target features label(that should be consider for later in model training)\n",
        "\n",
        "2-we have un-normalized distribution\n",
        "\n",
        "3-w\\seems we have no NOISE!\n",
        "\n",
        "4-for each feature, if two histograms are separate, this means that the feature is important and it directly affects the target (diagnosis). (e.g. mpacctrailsb_bl make a better sample distribute sepration by target labes) "
      ],
      "metadata": {
        "id": "rZEnX4YmFCdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**feature selection**:\n",
        "by this information we can do feature selection. "
      ],
      "metadata": {
        "id": "7lrnHzsqFCdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df6_s = df6[['midtemp_bl', 'fusiform_bl', 'entorhinal_bl', 'hippocampus_bl',\n",
        "       'ventricles_bl', 'imageuid_bl',\n",
        "       'trabscor_bl', 'faq_bl', 'adas13_bl', 'ravlt_learning_bl', 'ravlt_immediate_bl',\n",
        "       'adas11_bl', 'mpacctrailsb_bl', 'mpaccdigit_bl',\n",
        "       'ldeltotal_bl', 'mmse_bl', 'adasq4_bl', 'cdrsb_bl', 'ravlt_perc_forgetting_bl','target']]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.245919Z",
          "iopub.execute_input": "2022-09-30T14:37:45.247201Z",
          "iopub.status.idle": "2022-09-30T14:37:45.255394Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.247148Z",
          "shell.execute_reply": "2022-09-30T14:37:45.254121Z"
        },
        "trusted": true,
        "id": "dSAiEL8NFCdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 1 — Training the Model using all the Features**:\n",
        "\n",
        "Before we perform PCA on the dataset, let’s use some classifiers to train a model using all features in the dataset and see how well it performs:"
      ],
      "metadata": {
        "id": "gutnIQc2FCdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert target numeric values\n",
        "df6_s['target'].replace(['HCL', 'mMCI','sMCI','pMCI','AD'],\n",
        "                        [1, 2, 3, 4, 5], inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.2576Z",
          "iopub.execute_input": "2022-09-30T14:37:45.258586Z",
          "iopub.status.idle": "2022-09-30T14:37:45.271257Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.258538Z",
          "shell.execute_reply": "2022-09-30T14:37:45.270162Z"
        },
        "trusted": true,
        "id": "1aVbmDe5FCdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove missingvalues\n",
        "#NOTE: we usully need to use better method(contact with data owner, used refill method,...)\n",
        "#but in this case seems drop all missing values doesnt compromised data.\n",
        "df7 = df6_s.dropna()\n",
        "df7.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.272808Z",
          "iopub.execute_input": "2022-09-30T14:37:45.273647Z",
          "iopub.status.idle": "2022-09-30T14:37:45.28369Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.2736Z",
          "shell.execute_reply": "2022-09-30T14:37:45.282228Z"
        },
        "trusted": true,
        "id": "x7IXRLtjFCdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df7.drop(['target'],axis = 1)#features\n",
        "y = df7['target']#target"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.29029Z",
          "iopub.execute_input": "2022-09-30T14:37:45.29074Z",
          "iopub.status.idle": "2022-09-30T14:37:45.297439Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.290702Z",
          "shell.execute_reply": "2022-09-30T14:37:45.296056Z"
        },
        "trusted": true,
        "id": "AACHT9d4FCdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split into validation and training data\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.299332Z",
          "iopub.execute_input": "2022-09-30T14:37:45.299834Z",
          "iopub.status.idle": "2022-09-30T14:37:45.30764Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.299799Z",
          "shell.execute_reply": "2022-09-30T14:37:45.306381Z"
        },
        "trusted": true,
        "id": "LyXwB5DbFCdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Define the model.\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# fit your model\n",
        "nb_model.fit(train_X, train_y.ravel())\n",
        "y_pred = nb_model.predict(val_X)\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.309162Z",
          "iopub.execute_input": "2022-09-30T14:37:45.310347Z",
          "iopub.status.idle": "2022-09-30T14:37:45.332729Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.310274Z",
          "shell.execute_reply": "2022-09-30T14:37:45.331496Z"
        },
        "trusted": true,
        "id": "3uZiXsatFCdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the model.\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "# fit your model\n",
        "rf_model.fit(train_X,train_y)\n",
        "val_predictions = rf_model.predict(val_X)\n",
        "y_pred = rf_model.predict(val_X)\n",
        "\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:45.334499Z",
          "iopub.execute_input": "2022-09-30T14:37:45.335189Z",
          "iopub.status.idle": "2022-09-30T14:37:46.166266Z",
          "shell.execute_reply.started": "2022-09-30T14:37:45.335145Z",
          "shell.execute_reply": "2022-09-30T14:37:46.16527Z"
        },
        "trusted": true,
        "id": "COG46OPvFCdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 2 — Training the Model using Reduced Features:**\n",
        "\n",
        "For the next method, let’s examine the various features and try to eliminate those features that are least correlated to the target. At the same time, we also want to remove those features that exhibit muli-collinearity. The aim is to reduce the number of features and see if the accuracy of the model can be improved."
      ],
      "metadata": {
        "id": "E6_6gZpuFCdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the Correlation Factors:\n",
        "\n",
        "Let’s first get the correlation of each feature with respect to the target:"
      ],
      "metadata": {
        "id": "edNb5z-VFCdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr = df7.corr()['target'].abs().sort_values(ascending=False)\n",
        "df_corr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.167508Z",
          "iopub.execute_input": "2022-09-30T14:37:46.169004Z",
          "iopub.status.idle": "2022-09-30T14:37:46.183636Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.168945Z",
          "shell.execute_reply": "2022-09-30T14:37:46.18185Z"
        },
        "trusted": true,
        "id": "vaX8GiiKFCdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then extract all those features that have relatively high correlation to the target (we arbitrarily set the threshold to 0.6):"
      ],
      "metadata": {
        "id": "BidqLWAJFCdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get all the features that has at least 0.6 in correlation to the \n",
        "# target\n",
        "features = df_corr[df_corr > 0.6].index.to_list()[1:]\n",
        "features                          # without the 'target' column"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.185293Z",
          "iopub.execute_input": "2022-09-30T14:37:46.185764Z",
          "iopub.status.idle": "2022-09-30T14:37:46.195016Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.185729Z",
          "shell.execute_reply": "2022-09-30T14:37:46.193418Z"
        },
        "trusted": true,
        "id": "EPj4e3koFCdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for MultiCollinearity**\n",
        "\n",
        "Let’s remove those features that exhibits multi-collinearity:"
      ],
      "metadata": {
        "id": "p9fj4JQyFCdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def calculate_vif(df7, features):   \n",
        "    \n",
        "    vif, tolerance = {}, {}    # all the features that you want to examine\n",
        "    for feature in features:\n",
        "        # extract all the other features you will regress against\n",
        "        X = [f for f in features if f != feature]        \n",
        "        X, y = df7[X], df7[feature]        # extract r-squared from the fit\n",
        "        r2 = LinearRegression().fit(X, y).score(X, y)                \n",
        "        \n",
        "        # calculate tolerance\n",
        "        tolerance[feature] = 1 - r2        # calculate VIF\n",
        "        vif[feature] = 1/(tolerance[feature])\n",
        "    # return VIF DataFrame\n",
        "    return pd.DataFrame({'VIF': vif, 'Tolerance': tolerance})\n",
        "\n",
        "calculate_vif(df7,features)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.196642Z",
          "iopub.execute_input": "2022-09-30T14:37:46.197032Z",
          "iopub.status.idle": "2022-09-30T14:37:46.263045Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.196997Z",
          "shell.execute_reply": "2022-09-30T14:37:46.261771Z"
        },
        "trusted": true,
        "id": "RCh_fdGjFCdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "our aim would be to remove those features that have VIF greater than 5. we can iteratively call the calculate_vif() function with different features until we have a feature-set that has all VIF values lesser than 5."
      ],
      "metadata": {
        "id": "Pu7a2eMnFCdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try to reduce those feature that has high VIF until each feature \n",
        "# has VIF less than 5\n",
        "features = [\n",
        "    'cdrsb_bl',\n",
        " 'ldeltotal_bl',\n",
        " 'adas13_bl'\n",
        "]\n",
        "\n",
        "calculate_vif(df7,features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.264948Z",
          "iopub.execute_input": "2022-09-30T14:37:46.265557Z",
          "iopub.status.idle": "2022-09-30T14:37:46.297561Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.2655Z",
          "shell.execute_reply": "2022-09-30T14:37:46.296345Z"
        },
        "trusted": true,
        "id": "Nh3l0meBFCdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model\n",
        "\n",
        "With the 22 features reduced to 3, let’s now train the model"
      ],
      "metadata": {
        "id": "vYRs40ffFCdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df7[[\n",
        "    'cdrsb_bl',\n",
        " 'ldeltotal_bl',\n",
        " 'adas13_bl'\n",
        "]]\n",
        "y = df7['target']\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.299065Z",
          "iopub.execute_input": "2022-09-30T14:37:46.299505Z",
          "iopub.status.idle": "2022-09-30T14:37:46.307452Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.299468Z",
          "shell.execute_reply": "2022-09-30T14:37:46.305449Z"
        },
        "trusted": true,
        "id": "mLbPHtooFCdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into validation and training data\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.309447Z",
          "iopub.execute_input": "2022-09-30T14:37:46.30986Z",
          "iopub.status.idle": "2022-09-30T14:37:46.319867Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.309827Z",
          "shell.execute_reply": "2022-09-30T14:37:46.318256Z"
        },
        "trusted": true,
        "id": "5WK3oBXWFCdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Define the model.\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# fit your model\n",
        "nb_model.fit(train_X, train_y.ravel())\n",
        "y_pred = nb_model.predict(val_X)\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.321904Z",
          "iopub.execute_input": "2022-09-30T14:37:46.322261Z",
          "iopub.status.idle": "2022-09-30T14:37:46.340697Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.322229Z",
          "shell.execute_reply": "2022-09-30T14:37:46.339385Z"
        },
        "trusted": true,
        "id": "fZrjIk2IFCdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split into validation and training data\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
        "\n",
        "# Define the model. Set random_state to 1\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "# fit your model\n",
        "rf_model.fit(train_X,train_y)\n",
        "val_predictions = rf_model.predict(val_X)\n",
        "\n",
        "y_pred = rf_model.predict(val_X)\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.343193Z",
          "iopub.execute_input": "2022-09-30T14:37:46.343601Z",
          "iopub.status.idle": "2022-09-30T14:37:46.617653Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.343566Z",
          "shell.execute_reply": "2022-09-30T14:37:46.616373Z"
        },
        "trusted": true,
        "id": "ALUmm5ThFCdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that the Naive Bayes Model improved but the Random Forrest not!"
      ],
      "metadata": {
        "id": "dAU5h_13FCdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Principal Component Analysis (PCA):**\n",
        "\n",
        "In short, PCA is a dimensionality reduction technique that transforms a set of features in a dataset into a smaller number of features called principal components while at the same time trying to retain as much information in the original dataset as possible."
      ],
      "metadata": {
        "id": "AwnWSxw9FCdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method 3 — Training the Model using Reduced Features (PCA):**\n",
        "\n",
        "let’s apply PCA to the dataset and see if a better model can be trained.\n",
        "Performing Standard Scaling\n",
        "\n",
        "Remember that PCA is sensitive to scaling? So the first step is to perform a standard scaling on the features:"
      ],
      "metadata": {
        "id": "XJDqy2enFCdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# get the features and label from the original dataframe\n",
        "X = df7.iloc[:,:-1]\n",
        "y = df7.iloc[:,-1]\n",
        "\n",
        "# performing standardization\n",
        "sc = StandardScaler()\n",
        "X_scaled = sc.fit_transform(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.619018Z",
          "iopub.execute_input": "2022-09-30T14:37:46.619459Z",
          "iopub.status.idle": "2022-09-30T14:37:46.630624Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.619429Z",
          "shell.execute_reply": "2022-09-30T14:37:46.629329Z"
        },
        "trusted": true,
        "id": "5551QpxtFCdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Principal Component Analysis (PCA)**\n",
        "\n",
        "we can now apply PCA to the features using the PCA class in the sklearn.decomposition module:"
      ],
      "metadata": {
        "id": "pgVE32sQFCdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now apply PCA to find the desired number of components based on the desired explained variance, say 85%:"
      ],
      "metadata": {
        "id": "8XNFFqjwFCdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components = 0.85)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Cumulative Variances (Percentage):\")\n",
        "print(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
        "\n",
        "components = len(pca.explained_variance_ratio_)\n",
        "print(f'Number of components: {components}')\n",
        "\n",
        "# Make the scree plot\n",
        "plt.plot(range(1, components + 1), \n",
        "np.cumsum(pca.explained_variance_ratio_ * 100))\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"Explained variance (%)\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.632693Z",
          "iopub.execute_input": "2022-09-30T14:37:46.633369Z",
          "iopub.status.idle": "2022-09-30T14:37:46.918892Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.633319Z",
          "shell.execute_reply": "2022-09-30T14:37:46.917838Z"
        },
        "trusted": true,
        "id": "jEdTR8VVFCda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And as we can see from the chart, 9 components are needed to cover 85% of the variability in the data.\n",
        "\n",
        "we can also find out the importance of each feature that contributes to each of the components using the components_ attribute of the pca object:"
      ],
      "metadata": {
        "id": "A1-kqvqTFCda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_components = abs(pca.components_)\n",
        "#print(pca_components)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.920554Z",
          "iopub.execute_input": "2022-09-30T14:37:46.92157Z",
          "iopub.status.idle": "2022-09-30T14:37:46.926456Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.92153Z",
          "shell.execute_reply": "2022-09-30T14:37:46.925291Z"
        },
        "trusted": true,
        "id": "OvrGE7Q_FCda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Top 4 most important features in each component')\n",
        "#print('===============================================')\n",
        "#for row in range(pca_components.shape[0]):\n",
        "    # get the indices of the top 4 values in each row\n",
        "   # temp = np.argpartition(-(pca_components[row]), 4)\n",
        "    \n",
        "    # sort the indices in descending order\n",
        "  #  indices = temp[np.argsort((-pca_components[row])[temp])][:4]\n",
        "    \n",
        "    # print the top 4 feature names\n",
        " #   print(f'Component {row}: {df7.columns[indices].to_list()}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.928169Z",
          "iopub.execute_input": "2022-09-30T14:37:46.928609Z",
          "iopub.status.idle": "2022-09-30T14:37:46.937402Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.928572Z",
          "shell.execute_reply": "2022-09-30T14:37:46.936213Z"
        },
        "trusted": true,
        "id": "WZFGSiF2FCdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transforming all the 24 Columns to the 10 Principal Components**\n"
      ],
      "metadata": {
        "id": "h_S5mm6WFCdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca = pca.transform(X_scaled)\n",
        "#print(X_pca.shape)\n",
        "#print(X_pca)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.939018Z",
          "iopub.execute_input": "2022-09-30T14:37:46.939606Z",
          "iopub.status.idle": "2022-09-30T14:37:46.948721Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.939568Z",
          "shell.execute_reply": "2022-09-30T14:37:46.947596Z"
        },
        "trusted": true,
        "id": "TDBaFaK8FCdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Machine Learning Pipeline**\n",
        "\n",
        "Let’s now create a machine learning pipeline so that we can formalize the entire process:"
      ],
      "metadata": {
        "id": "qFEetOqHFCdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "_sc = StandardScaler()\n",
        "_pca = PCA(n_components = 0.85)\n",
        "_model = RandomForestClassifier(n_estimators=100)\n",
        "_model2 =GaussianNB()\n",
        "\n",
        "\n",
        "rf_model = Pipeline([\n",
        "    ('std_scaler', _sc),\n",
        "    ('pca', _pca),\n",
        "    ('Classifier', _model)\n",
        "])\n",
        "\n",
        "nb_model = Pipeline([\n",
        "    ('std_scaler', _sc),\n",
        "    ('pca', _pca),\n",
        "    ('Classifier', _model2)\n",
        "])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.951167Z",
          "iopub.execute_input": "2022-09-30T14:37:46.95218Z",
          "iopub.status.idle": "2022-09-30T14:37:46.963051Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.952143Z",
          "shell.execute_reply": "2022-09-30T14:37:46.961846Z"
        },
        "trusted": true,
        "id": "qCvk_ZNeFCdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform a split\n",
        "random_state = 12\n",
        "train_X, val_X, train_y, val_y = \\\n",
        "    train_test_split(X, y, \n",
        "                     test_size=0.3,\n",
        "                     shuffle=True, \n",
        "                     random_state=random_state)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.96507Z",
          "iopub.execute_input": "2022-09-30T14:37:46.965505Z",
          "iopub.status.idle": "2022-09-30T14:37:46.974941Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.965459Z",
          "shell.execute_reply": "2022-09-30T14:37:46.973769Z"
        },
        "trusted": true,
        "id": "ppPtIrUKFCdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit your model\n",
        "nb_model.fit(train_X, train_y.ravel())\n",
        "y_pred = nb_model.predict(val_X)\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:46.976421Z",
          "iopub.execute_input": "2022-09-30T14:37:46.976806Z",
          "iopub.status.idle": "2022-09-30T14:37:47.006693Z",
          "shell.execute_reply.started": "2022-09-30T14:37:46.976775Z",
          "shell.execute_reply": "2022-09-30T14:37:47.004767Z"
        },
        "trusted": true,
        "id": "a3zDpEH4FCdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model using the PCA components\n",
        "rf_model.fit(train_X,train_y)\n",
        "val_predictions = rf_model.predict(val_X)\n",
        "\n",
        "y_pred = rf_model.predict(val_X)\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:47.008914Z",
          "iopub.execute_input": "2022-09-30T14:37:47.009591Z",
          "iopub.status.idle": "2022-09-30T14:37:47.477347Z",
          "shell.execute_reply.started": "2022-09-30T14:37:47.00954Z",
          "shell.execute_reply": "2022-09-30T14:37:47.476025Z"
        },
        "trusted": true,
        "id": "W5yc14YXFCdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see, PCA cant help to improve neither Naive Bayes nor Random Forest."
      ],
      "metadata": {
        "id": "CEeirC-vFCde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Improving model use other methods**"
      ],
      "metadata": {
        "id": "1AML3wfrFCde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use KFOLD for a better data selection**"
      ],
      "metadata": {
        "id": "vHn_yRjvFCde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df7.drop(['target'],axis = 1)#features\n",
        "y = df7['target']#target"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:47.479113Z",
          "iopub.execute_input": "2022-09-30T14:37:47.479911Z",
          "iopub.status.idle": "2022-09-30T14:37:47.487405Z",
          "shell.execute_reply.started": "2022-09-30T14:37:47.479864Z",
          "shell.execute_reply": "2022-09-30T14:37:47.485975Z"
        },
        "trusted": true,
        "id": "W8Q_itfHFCdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into validation and training data\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:47.489492Z",
          "iopub.execute_input": "2022-09-30T14:37:47.48997Z",
          "iopub.status.idle": "2022-09-30T14:37:47.501491Z",
          "shell.execute_reply.started": "2022-09-30T14:37:47.489924Z",
          "shell.execute_reply": "2022-09-30T14:37:47.499904Z"
        },
        "trusted": true,
        "id": "QW-iKKJVFCdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the model.\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "# fit your model\n",
        "rf_model.fit(train_X,train_y)\n",
        "val_predictions = rf_model.predict(val_X)\n",
        "y_pred = rf_model.predict(val_X)\n",
        "\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:47.502921Z",
          "iopub.execute_input": "2022-09-30T14:37:47.503286Z",
          "iopub.status.idle": "2022-09-30T14:37:47.89077Z",
          "shell.execute_reply.started": "2022-09-30T14:37:47.503254Z",
          "shell.execute_reply": "2022-09-30T14:37:47.889679Z"
        },
        "trusted": true,
        "id": "K0cOCJhPFCdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k_fold = KFold(20)\n",
        "\n",
        "print(cross_val_score(rf_model,X,y.ravel(), cv=k_fold, n_jobs=1))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:47.892005Z",
          "iopub.execute_input": "2022-09-30T14:37:47.892927Z",
          "iopub.status.idle": "2022-09-30T14:37:56.328664Z",
          "shell.execute_reply.started": "2022-09-30T14:37:47.89289Z",
          "shell.execute_reply": "2022-09-30T14:37:56.327361Z"
        },
        "trusted": true,
        "id": "A6KH4x78FCdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_n = df7.drop(df.index[0:463])\n",
        "df_n = df_n.drop(df.index[528:596])\n",
        "df_n = df_n.drop(df.index[990:1057])\n",
        "df_n = df_n.drop(df.index[1240:1306])\n",
        "df_n.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:56.330603Z",
          "iopub.execute_input": "2022-09-30T14:37:56.331059Z",
          "iopub.status.idle": "2022-09-30T14:37:56.341668Z",
          "shell.execute_reply.started": "2022-09-30T14:37:56.331015Z",
          "shell.execute_reply": "2022-09-30T14:37:56.340375Z"
        },
        "trusted": true,
        "id": "OBW6XsaUFCdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_n.drop(['target'],axis = 1)#features\n",
        "y = df_n['target']#target"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:56.343091Z",
          "iopub.execute_input": "2022-09-30T14:37:56.343581Z",
          "iopub.status.idle": "2022-09-30T14:37:56.351668Z",
          "shell.execute_reply.started": "2022-09-30T14:37:56.343545Z",
          "shell.execute_reply": "2022-09-30T14:37:56.350568Z"
        },
        "trusted": true,
        "id": "6aFJj1FEFCdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into validation and training data\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:56.359694Z",
          "iopub.execute_input": "2022-09-30T14:37:56.360082Z",
          "iopub.status.idle": "2022-09-30T14:37:56.367324Z",
          "shell.execute_reply.started": "2022-09-30T14:37:56.36005Z",
          "shell.execute_reply": "2022-09-30T14:37:56.366052Z"
        },
        "trusted": true,
        "id": "aKf-oD1tFCdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Define the model.\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# fit your model\n",
        "nb_model.fit(train_X, train_y.ravel())\n",
        "y_pred = nb_model.predict(val_X)\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:56.369075Z",
          "iopub.execute_input": "2022-09-30T14:37:56.36949Z",
          "iopub.status.idle": "2022-09-30T14:37:56.389248Z",
          "shell.execute_reply.started": "2022-09-30T14:37:56.369439Z",
          "shell.execute_reply": "2022-09-30T14:37:56.388014Z"
        },
        "trusted": true,
        "id": "94C2wMECFCdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the model.\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "# fit your model\n",
        "rf_model.fit(train_X,train_y)\n",
        "val_predictions = rf_model.predict(val_X)\n",
        "y_pred = rf_model.predict(val_X)\n",
        "\n",
        "\n",
        "#---evaluate the model---\n",
        "print('Model Accuracy:\\n', metrics.accuracy_score(val_y,y_pred))\n",
        "\n",
        "print('confusion_matrix:')\n",
        "print(metrics.confusion_matrix(val_y,y_pred))\n",
        "\n",
        "print('f1_score:\\n', metrics.f1_score(val_y,y_pred, average='macro'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-30T14:37:56.39121Z",
          "iopub.execute_input": "2022-09-30T14:37:56.391702Z",
          "iopub.status.idle": "2022-09-30T14:37:56.657749Z",
          "shell.execute_reply.started": "2022-09-30T14:37:56.391657Z",
          "shell.execute_reply": "2022-09-30T14:37:56.656541Z"
        },
        "trusted": true,
        "id": "bynkkKOXFCdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "by extracting better data we improve Model accuracy up to 85."
      ],
      "metadata": {
        "id": "U23mGDLMFCdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "In recap: \n",
        "in this study we tried to train a Classifier Machine Learning Model to predict the dementia progression in Mild Cognitive Impairment patients and getting Alzheimer from healthy participants. We used just non-invasive features like medical imaging and cognitive test scores. After data cleanation and analysis, we traind to model, a Naive Bayes model and a Random Forest one. Then we tryed different technique to improve the models like, feature selection, feature reduction( principal component analysis(PCA)), and Kfold data extraction. Finally its seems that Random Forest Classifier is the best model with about 85 accuracy.\n"
      ],
      "metadata": {
        "id": "1QdFEx8zFCdj"
      }
    }
  ]
}